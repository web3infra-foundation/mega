{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e370d3",
   "metadata": {},
   "source": [
    "# Parallel Training of One Vs. All Logistic Regression Classifiers\n",
    "\n",
    "In this example, we will show how dagrs can help implement machine learning algorithms to train classifiers in a parallel manner.\n",
    "\n",
    "Install required packages if you need to run the following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeae6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes --file requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1ae81",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Feel free to skip this section if you are an experienced machine learning expert.\n",
    "This section helps you understand the concept of logistic regression and one vs. all classifiers. \n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression (LR) is a statistical method and machine learning (ML) algorithm used for binary classification tasks, where the goal is to predict one of two possible outcomes based on input data.\n",
    "\n",
    "The core of logistic regression is the sigmoid function (also called the logistic function), which maps any input to a value between 0 and 1. This output can be interpreted as a probability that an instance belongs to a particular class.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "$$\n",
    "    \\delta(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "The output $\\delta(z)$ represents the probability of the instance belonging to class 1. So the probability of belonging to class 0 is 1-$\\delta(z)$.\n",
    "\n",
    "Let's implement function $\\delta(z)$ in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2378756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27dee3",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "Logistic regression uses a log-likelihood function (also known as binary cross-entropy) to measure how well the model's predictions match the actual labels. The goal is to minimize this cost function through optimization (usually using gradient descent).\n",
    "\n",
    "The binary cross-entropy loss for a single training example is:\n",
    "$$\n",
    "L(y,\\hat{y})=-[ylog(\\hat{y})+(1-y)log(1-\\hat{y})]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is the actual class (either 0 or 1),\n",
    "- $\\hat{y}$ is the predicted probability of the instance being in class 1.\n",
    "\n",
    "Assume $\\theta$ represents the parameters in our model, then the cost function of $\\theta$ is\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(\\delta(\\theta x^{(i)}))+(1-y)^{(i)}log(1-\\delta(\\theta x^{(i)}))] + \\frac{\\lambda}{2m}\\sum^n_{i=1}\\theta_j^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter. It is used to prevent overfitting by penalizing large values of the model's parameters.\n",
    "\n",
    "Now let's implement function $J(\\theta)$ in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(-y*np.log(sigmoid(X@Theta)))-(1-y)*np.log(1-sigmoid((X@Theta)))\n",
    "    reg=(ThetaReg@ThetaReg)*l/(2*len(X))\n",
    "    return np.mean(cost)+reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870aecc9",
   "metadata": {},
   "source": [
    "### Gradient Function\n",
    "\n",
    "The gradient function refers to the vector of partial derivatives of a function with respect to its parameters. In machine learning and optimization, the gradient is used to update model parameters during training (e.g., in gradient descent) in order to minimize a cost or loss function.\n",
    "\n",
    "The gradient of $J(\\theta)$ with respect to $\\theta_j$ is computed as follows:\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0}=\\frac{1}{m}\\sum^m_{i=1}(\\delta(\\theta x^{(i)}) - y^{(i)})x_0^{(i)}, \\text{for $j = 0$},\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=(\\frac{1}{m}\\sum^m_{i=1}(\\delta(\\theta x^{(i)}) - y^{(i)})x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j, \\text{for $j \\ge 1$}.\n",
    "$$\n",
    "\n",
    "Next let's implement the gradient function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c90095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_gradient(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(X.T@(sigmoid(X@Theta)-y))*(1/len(X))\n",
    "    reg=np.concatenate([np.array([0]),(l/len(X))*ThetaReg])\n",
    "    return cost+reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95941b",
   "metadata": {},
   "source": [
    "### One-vs-All Classifier\n",
    "\n",
    "The One-vs-All (OvA) classifier, also known as One-vs-Rest (OvR), is a strategy used to extend binary classification algorithms to handle multi-class classification problems. It is commonly used in algorithms like logistic regression, Support Vector Machines, and others when you need to classify data into more than two classes.\n",
    "\n",
    "For a classification problem with $K$ classes, the OvA strategy trains $K$ separate binary classifiers.For each classifier $k$ (where $k = 1, 2, ..., K$), the classifier is trained to distinguish between class $k$ and all other classes.\n",
    "\n",
    "For example, if you have a multi-class problem with classes A, B, and C, you'll train three classifiers:\n",
    "- Classifier 1: Class A vs. (Class B $\\cup$ Class C)\n",
    "- Classifier 2: Class B vs. (Class A $\\cup$ Class C)\n",
    "- Classifier 3: Class C vs. (Class A $\\cup$ Class B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203e943",
   "metadata": {},
   "source": [
    "## Training Set\n",
    "\n",
    "`ex3data1.mat` contains 5000 images of handwritten digits. It is a dataset typically used in machine learning exercises.\n",
    "The following code displays 100 images in this dataset randomly (this may take a while). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def load_data(path):\n",
    "    data=loadmat(path)\n",
    "    X=data[\"X\"]\n",
    "    y=data[\"y\"]\n",
    "    return X,y\n",
    "\n",
    "def plot_an_image(X):\n",
    "    # Pick 100 images randomly\n",
    "    sample_index=np.random.choice(np.arange(X.shape[0]),100)\n",
    "    sample_imagex=X[sample_index,:]\n",
    "    #image (100:400)\n",
    "    fix,ax_array=plt.subplots(nrows=10,ncols=10,figsize=(8,8),sharey=True,sharex=True)\n",
    "    for row in range(10):\n",
    "        for col in range(10):\n",
    "            ax_array[row,col].matshow(sample_imagex[row*10+col].reshape(20,20),cmap='gray_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "X, y = load_data('ex3data1.mat')\n",
    "plot_an_image(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85c1c1",
   "metadata": {},
   "source": [
    "## Traditional Approach to LR OvA Solutions\n",
    "\n",
    "This sections shows how we write a ML algoritm in python to recognize these handwritten digits above.\n",
    "\n",
    "We will use multiple logistic regression models to build a multi-class classifier. Because there are 10 classes (i.e. from digit 1 to digit 10), we need 10 separate logistic regression classifiers.\n",
    "\n",
    "The [following code](./raw.py) trains 10 different classifiers (in function `one_vs_all`), and then prints prediction accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823281b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import sys\n",
    "\n",
    "def one_vs_all(X,y,l,K):\n",
    "    All_Theta=np.zeros((K,X.shape[1]))\n",
    "    for i in range(1,K+1):\n",
    "        Theta=np.zeros(X.shape[1])\n",
    "        y_i=np.array([1 if labal==i else 0 for labal in y])\n",
    "        ret=minimize(fun=regularized_cost, x0=Theta,args=(X,y_i,l),method='TNC',jac=regularized_gradient)\n",
    "        All_Theta[i-1:]=ret.x\n",
    "    return All_Theta\n",
    "\n",
    "def load_data(path):\n",
    "    data=loadmat(path)\n",
    "    X=data[\"X\"]\n",
    "    y=data[\"y\"]\n",
    "    return X,y\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def regularized_cost(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(-y*np.log(sigmoid(X@Theta)))-(1-y)*np.log(1-sigmoid((X@Theta)))\n",
    "    reg=(ThetaReg@ThetaReg)*l/(2*len(X))\n",
    "    return np.mean(cost)+reg\n",
    "\n",
    "def regularized_gradient(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(X.T@(sigmoid(X@Theta)-y))*(1/len(X))\n",
    "    reg=np.concatenate([np.array([0]),(l/len(X))*ThetaReg])\n",
    "    return cost+reg\n",
    "\n",
    "def predict(X,All_Theta):\n",
    "    h=sigmoid(X@All_Theta.T)\n",
    "    h_argmax=np.argmax(h,axis=1)\n",
    "    h_argmax=h_argmax+1\n",
    "    return h_argmax\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "X, y = load_data('ex3data1.mat')\n",
    "X=np.insert(X,0,1,axis=1)\n",
    "y=y.flatten()\n",
    "All_Theta=one_vs_all(X, y, 1, 10)\n",
    "y_predict=predict(X, All_Theta)\n",
    "accuracy=np.mean(y_predict==y)\n",
    "print(\"accuracy=%.2f%%\"%(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fefcae",
   "metadata": {},
   "source": [
    "## Train Classifiers in a Parallel Manner\n",
    "\n",
    "If the training set is much larger than we use here, then training each classifier one by one will take a long time.\n",
    "\n",
    "We can use dagrs to train these models in parallel.\n",
    "\n",
    "First, create 10 nodes and train a model on each node. Then create a 'root' node collects the training results.\n",
    "\n",
    "We only use dagrs for shceduling the training and predicting tasks. The [training](./lr_i.py) and [prediction](./lr_root.py) logic will still be written in python scripts.\n",
    "\n",
    "### Training Script of Each Class\n",
    "The training [script](./lr_i.py) receives 2 parameters: the training set file path and the class $i$. It calculates the converged $theta_i$, saves it to a file, and prints the path to the saved file to stdout.\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    data=loadmat(path)\n",
    "    X=data[\"X\"]\n",
    "    y=data[\"y\"]\n",
    "    return X,y\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def regularized_cost(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(-y*np.log(sigmoid(X@Theta)))-(1-y)*np.log(1-sigmoid((X@Theta)))\n",
    "    reg=(ThetaReg@ThetaReg)*l/(2*len(X))\n",
    "    return np.mean(cost)+reg\n",
    "\n",
    "\n",
    "def regularized_gradient(Theta,X,y,l):\n",
    "    ThetaReg=Theta[1:]\n",
    "    cost=(X.T@(sigmoid(X@Theta)-y))*(1/len(X))\n",
    "    reg=np.concatenate([np.array([0]),(l/len(X))*ThetaReg])\n",
    "    return cost+reg\n",
    "\n",
    "def one_vs_all(X,y,l,i):\n",
    "    Theta=np.zeros(X.shape[1])\n",
    "    y_i=np.array([1 if labal==(i + 1) else 0 for labal in y])\n",
    "    ret=minimize(fun=regularized_cost, x0=Theta,args=(X,y_i,l),method='TNC',jac=regularized_gradient)\n",
    "    return ret.x\n",
    "\n",
    "argc = len(sys.argv)\n",
    "argv = sys.argv\n",
    "\n",
    "X, y = load_data(argv[1])\n",
    "X=np.insert(X,0,1,axis=1)\n",
    "y=y.flatten()\n",
    "\n",
    "i = int(argv[2])\n",
    "theta = (one_vs_all(X, y, 1, i))\n",
    "\n",
    "s = np.array2string(theta, max_line_width=1 << 31)\n",
    "with open(f\"theta_{i}.txt\", \"w\") as f:\n",
    "    f.write(s)\n",
    "print(f\"theta_{i}.txt\")\n",
    "```\n",
    "\n",
    "The [prediction](./lr_root.py) script receives 10 arguments, each of which is the path to a file containing $theta_i$. This script will merge each file and get the $theta$ we need. Finally, it will calculate the prediction accuracy on the training set and print the result to stdout.\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    data=loadmat(path)\n",
    "    X=data[\"X\"]\n",
    "    y=data[\"y\"]\n",
    "    return X,y\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(X,All_Theta):\n",
    "    h=sigmoid(X@All_Theta.T)\n",
    "    h_argmax=np.argmax(h,axis=1)\n",
    "    h_argmax=h_argmax+1\n",
    "    return h_argmax\n",
    "\n",
    "def main():\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    argv = sys.argv\n",
    "    X, y = load_data(argv[1])\n",
    "    X=np.insert(X,0,1,axis=1)\n",
    "    y=y.flatten()\n",
    "    \n",
    "    # Load theta values for each class from the provided files\n",
    "    All_Theta = np.zeros((10, X.shape[1]))  # 10 classes, with the same number of features\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        # Load theta for class (i+1) from the file\n",
    "        file = argv[2+i]\n",
    "        j = int(file[6])\n",
    "        with open(file, \"r\") as f:\n",
    "            s = f.read().strip(\"[] \")\n",
    "            All_Theta[j] = np.fromstring(s, sep=\" \", dtype=float)\n",
    "        os.remove(file)\n",
    "\n",
    "    # Predict the class using the provided theta values\n",
    "    y_predict = predict(X, All_Theta)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    y_predict=predict(X, All_Theta)\n",
    "    accuracy=np.mean(y_predict==y)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "### Build the Dagrs Graph\n",
    "We use a [yml config file](./config.yml) to define nodes and edges. Each training node (node0, node1, ..., node9) is connected with node 'root'.\n",
    "\n",
    "```yml\n",
    "  dagrs:\n",
    "  node0:\n",
    "    name: \"node_0\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node1:\n",
    "    name: \"node_1\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node2:\n",
    "    name: \"node_2\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node3:\n",
    "    name: \"node_3\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node4:\n",
    "    name: \"node_4\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node5:\n",
    "    name: \"node_5\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node6:\n",
    "    name: \"node_6\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node7:\n",
    "    name: \"node_7\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node8:\n",
    "    name: \"node_8\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  node9:\n",
    "    name: \"node_9\"\n",
    "    after: []\n",
    "    cmd: \"\"\n",
    "  root:\n",
    "    name: \"root\"\n",
    "    after: [\n",
    "      \"node0\",\n",
    "      \"node1\",\n",
    "      \"node2\",\n",
    "      \"node3\",\n",
    "      \"node4\",\n",
    "      \"node5\",\n",
    "      \"node6\",\n",
    "      \"node7\",\n",
    "      \"node8\",\n",
    "      \"node9\",\n",
    "    ]\n",
    "    cmd: \"\"\n",
    "```\n",
    "\n",
    "Training nodes will run a `NodeAction`, which starts the training python script with the argument class $i$ and a path to the training set file, waits for it and sends the script's stdout to root.\n",
    "\n",
    "```rust\n",
    "const ENV_DATA_SRC: &str = \"data_src\";\n",
    "\n",
    "struct NodeAction {\n",
    "    class: usize,\n",
    "}\n",
    "\n",
    "#[async_trait]\n",
    "impl Action for NodeAction {\n",
    "    async fn run(\n",
    "        &self,\n",
    "        _: &mut InChannels,\n",
    "        out_channels: &OutChannels,\n",
    "        env: Arc<EnvVar>,\n",
    "    ) -> Output {\n",
    "        let data_src: &&str = env.get_ref(ENV_DATA_SRC).unwrap();\n",
    "\n",
    "        let cmd_act = CommandAction::new(\n",
    "            \"python\",\n",
    "            vec![\n",
    "                \"examples/lr_i.py\".to_string(),\n",
    "                format!(\"{}\", data_src),\n",
    "                format!(\"{}\", self.class),\n",
    "            ]\n",
    "        );\n",
    "        let result = cmd_act.run(&mut InChannels::default(), &OutChannels::default(), env).await;\n",
    "        match result {\n",
    "            Output::Out(content) => {\n",
    "                let content: Arc<(Vec<String>, Vec<String>)> =\n",
    "                    content.unwrap().into_inner().unwrap();\n",
    "                let (stdout, _) = (&content.0, &content.1);\n",
    "\n",
    "                let theta = stdout.get(0).unwrap().clone();\n",
    "                out_channels.broadcast(Content::new(theta)).await\n",
    "            }\n",
    "            Output::Err(e) => panic!(\"{}\", e),\n",
    "            Output::ErrWithExitCode(code, content) => panic!(\n",
    "                \"Exit with {:?}, {:?}\",\n",
    "                code,\n",
    "                content.unwrap().get::<(Vec<String>, Vec<String>)>()\n",
    "            ),\n",
    "        };\n",
    "        Output::empty()\n",
    "    }\n",
    "}\n",
    "\n",
    "impl NodeAction {\n",
    "    fn new(class: usize) -> Self {\n",
    "        Self { class }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "For the root node, it will execute the logic encapsulated in `RootAction`, which will wait all the training nodes to finish, and send training nodes' output to the root python script.\n",
    "\n",
    "```rust\n",
    "struct RootAction;\n",
    "#[async_trait]\n",
    "impl Action for RootAction {\n",
    "    async fn run(&self, in_channels: &mut InChannels, _: &OutChannels, env: Arc<EnvVar>) -> Output {\n",
    "        let data_src: &&str = env.get_ref(ENV_DATA_SRC).unwrap();\n",
    "        let mut thetas: Vec<String> = in_channels\n",
    "            .map(|theta| {\n",
    "                let theta = theta.unwrap();\n",
    "                let theta = theta.get::<String>().unwrap();\n",
    "                theta.clone()\n",
    "            })\n",
    "            .await;\n",
    "\n",
    "        let mut args = vec![\"examples/lr_root.py\".to_string(), format!(\"{}\", data_src)];\n",
    "\n",
    "        args.append(&mut thetas);\n",
    "\n",
    "        let cmd_action = CommandAction::new(\"python\", args);\n",
    "        cmd_action\n",
    "            .run(&mut InChannels::default(), &OutChannels::default(), env)\n",
    "            .await\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340debb",
   "metadata": {},
   "source": [
    "Run the cell below and you will get the same result with the [traditional approach](#Traditional-Approach-to-LR-OvA-Solutions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && cargo run --example sklearn --release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976f17e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This example shows how to use dagrs to parallelize the training of multiple OvA classifiers. The way parameters are passed between nodes can be optimized. You can also replace the python script with Rust code if higher performance is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
